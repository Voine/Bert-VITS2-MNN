# Bert-VITS2-MNN

> âœ¨ A high-performance on-device TTS system for Chinese, powered by distilled Bert + Bert-VITS2 + MNN.

![banner](assets/banner.png) <!-- å¯ä»¥æ›¿æˆä½ çš„é¡µé¢å›¾ -->

---

## ğŸ§  Introduction

This project brings Bert-VITS2 to Android, re-implemented using:

- ğŸ§  **Distilled Chinese BERT** for lightweight semantic embedding
- ğŸ— **MNN** for efficient on-device inference
- ğŸ§¹ **cppjieba** and **cpptokenizer** for fast native text preprocessing

All components run **entirely offline** on Android. No server or internet required.

---

## ğŸ”¬ Architecture

```
Input Text
   â†“
Tokenization + G2P (cppjieba + tokenizer + kotlin code)
   â†“
BERT embedding (distilled Chinese model)
   â†“
Encoder + DP/SDP + Flow + Decoder (MNN)
   â†“
Waveform output (.wav)
```

---

## ğŸµ Demo Audios

Here are some sample results:

| Text               | Character | Audio                                      |
|--------------------|-----------|--------------------------------------------|
| åšå£«ï¼Œå½“åˆåœ¨é¾™é—¨ï¼Œæˆ‘ä¸è¯¥æ”¾ä½ èµ°çš„ã€‚  | Chen      | ğŸ”Š [Play](./wav_sample/output_chen.wav)    |
| æ—…è¡Œè€…ï¼Œå¥½ä¹…ä¸è§ã€‚          | Faruzan   | ğŸ”Š [Play](./wav_sample/output_faruzan.wav) |
| å·¥ä½œè¿˜æ²¡æœ‰åšå®Œï¼Œåˆè¦å¼€å§‹æ¬ç –äº†ã€‚   | Ganyu     | ğŸ”Š [Play](./wav_sample/output_ganyu.wav)   |

---

## âš¡ Quick Start

### Clone with submodules

```bash
git clone --recurse-submodules https://github.com/yourname/Bert-VITS2-MNN.git
cd Bert-VITS2-MNN
```

If already cloned:

```bash
git submodule update --init --recursive
```

### Build for Android

> ğŸ“¦ Requires NDK r25+, CMake 3.22+, Android Studio Arctic Fox+

```bash
# From project root
./gradlew assembleRelease
```

---

## ğŸ› Git LFS (for large models)

This repo uses Git LFS for `.mnn` and `.wav` files.

```bash
git lfs install
git lfs pull
```

To track files (if contributing):

```bash
git lfs track "*.mnn"
git lfs track "*.wav"
```

---

## ğŸ› ï¸ Dependencies (via Submodule)

| Library | Path |
|---------|------|
| MNN     | `third_party/MNN` |
| cppjieba| `third_party/cppjieba` |
| cpptokenizer | `third_party/cpptokenizer` |

To update or verify:

```bash
git submodule update --init --recursive
```

---

## ğŸ’¡ Model Distillation

We distilled a compact BERT encoder from `chinese-roberta-wwm-ext-large` using hidden state alignment and simplified tokenizer vocab.

Details coming in [`distill/distill.md`](distill/distill.md).

---

## ğŸŒ Multilingual README

You can switch to the ä¸­æ–‡ç‰ˆ [è¿™é‡Œ](README.zh.md) ã€‚

---

## ğŸ“‹ Project Layout

```
app/
â”œâ”€â”€ src/main/                 
â”‚           â”œâ”€â”€ assets               # mnn bert model, cppjieba dic, mnn bv2model
â”‚           â”œâ”€â”€ java/preprocess      # Text preprocess code
â”œâ”€â”€ bertvits2/                       # Bert-VITS2 infer code
â”œâ”€â”€ cppjieba                         # cppjieba interface 
â”œâ”€â”€ cpptokenizer                     # cpptokenizer interface

```

---

## ğŸ™Œ Acknowledgements

This project is built on the shoulders of:

- [VITS](https://github.com/jaywalnut310/vits)
- [BertVITS2](https://github.com/fishaudio/Bert-VITS2)
- [MNN](https://github.com/alibaba/MNN)
- [cppjieba](https://github.com/yanyiwu/cppjieba)

---

Made with â¤ï¸ by [Voine] Â· License: MIT
