# Bert-VITS2-MNN

> âœ¨ A high-performance on-device TTS system for Chinese, powered by distilled BERT + VITS2 + MNN.

![banner](assets/banner.png) <!-- å¯ä»¥æ›¿æˆä½ çš„é¡µé¢å›¾ -->

---

## ğŸ§  Introduction

This project brings Bert-VITS2 to Android, re-implemented using:

- ğŸ§  **Distilled Chinese BERT** for lightweight semantic embedding
- ğŸ— **MNN** for efficient on-device inference
- ğŸ§¹ **cppjieba** and **cpptokenizer** for fast native text preprocessing

All components run **entirely offline** on Android. No server or internet required.

---

## ğŸ”¬ Architecture

```
Input Text
   â†“
Tokenization + G2P (cppjieba + tokenizer)
   â†“
BERT embedding (distilled Chinese model)
   â†“
Flow + Decoder (MNN)
   â†“
Waveform output (.wav)
```

---

## ğŸµ Demo Audios

Here are some sample results:

| Text | Audio |
|------|-------|
| ä½ å¥½ï¼Œæ¬¢è¿ä½¿ç”¨BertVITS2ã€‚| ğŸ”Š [Play](./samples/nihao.wav) |
| æˆ‘ä»¬çš„æ¨¡å‹åœ¨æ‰‹æœºä¸Šä¹Ÿèƒ½è¿è¡Œå¾—éå¸¸æµç•…ã€‚| ğŸ”Š [Play](./samples/smooth.wav) |

```html
<!-- Optional inline audio player -->
<audio controls>
  <source src="samples/nihao.wav" type="audio/wav">
</audio>
```

---

## âš¡ Quick Start

### Clone with submodules

```bash
git clone --recurse-submodules https://github.com/yourname/Bert-VITS2-MNN.git
cd Bert-VITS2-MNN
```

If already cloned:

```bash
git submodule update --init --recursive
```

### Build for Android

> ğŸ“¦ Requires NDK r25+, CMake 3.22+, Android Studio Arctic Fox+

```bash
# From project root
./gradlew assembleRelease
```

---

## ğŸ› Git LFS (for large models)

This repo uses Git LFS for `.mnn` and `.wav` files.

```bash
git lfs install
git lfs pull
```

To track files (if contributing):

```bash
git lfs track "*.mnn"
git lfs track "*.wav"
```

---

## ğŸ› ï¸ Dependencies (via Submodule)

| Library | Path | Commit |
|---------|------|--------|
| MNN     | `third_party/MNN` | pinned @ abc1234 |
| cppjieba| `third_party/cppjieba` | pinned @ def5678 |
| cpptokenizer | `third_party/cpptokenizer` | pinned @ ghi9012 |

To update or verify:

```bash
git submodule update --init --recursive
```

---

## ğŸ’¡ Model Distillation

We distilled a compact BERT encoder from `chinese-roberta-wwm-ext-large` using hidden state alignment and simplified tokenizer vocab.

Details coming in [`model/distill.md`](model/distill.md).

---

## ğŸŒ Multilingual README

You can switch to the ä¸­æ–‡ç‰ˆ [è¿™é‡Œ](README.zh.md) ã€‚

---

## ğŸ“‹ Project Layout

```
app/
â”œâ”€â”€ cpp/                 # Native code
â”‚   â”œâ”€â”€ tokenizer/       # cppjieba + cpptokenizer
â”‚   â”œâ”€â”€ tts_engine/      # MNN inference, flow, decoder
â”œâ”€â”€ assets/              # Pre-trained .mnn model files
â”œâ”€â”€ samples/             # Demo audio
```

---

## ğŸ™Œ Acknowledgements

This project is built on the shoulders of:

- [VITS](https://github.com/jaywalnut310/vits)
- [BertVITS2](https://github.com/fishaudio/Bert-VITS2)
- [MNN](https://github.com/alibaba/MNN)
- [cppjieba](https://github.com/yanyiwu/cppjieba)

---

Made with â¤ï¸ by [yourname] Â· License: MIT
